{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring usefulness of NLP features like POS, Named Entity Recognition, Coreference and Lemmatizaton using CoreNLP, Spacy and Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Sentence Completion: Probablistic Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of sentence completion in NLP is to predict missing words in a sentence or incomplete beginings and endings of a sentence. Example \"John _ Susan in the mall\". Here a missing word needs to be predicted such that the sentence is coherent and syntactically correct. Sentence completion task has numerous applications like in email reply generation, spell check applications, bot interaction, determining the correctness of automated machine translation etc. \n",
    "\n",
    "One of the most popular and simple method of sentence completion is using N gram models. For example in bi gram model we calculate the probability of pair of words in a corpus and this information is used to predict the probability of missing words in the test corpus. This method suffers from limitations, such as it does not capture the relation the missing word(s) might have with words other than immediate words in a sentence. For example in a sentence \"I saw a tiger which was really very _ \". The missing word could be either fierce or talkative. If a bi gram model is used here we will see the P(fierce| very)  and P(talkative|very) which does not take into account the object \"tiger\" for which the adjective needs to be predicted.\n",
    "I will briefly describe how NLP tasks like POS tagging, Named entity, dependency parsing are useful for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of Speech Tagging: Given a corpus, we first get part of speech tag for each word in the corpus. Using this we can build a Hidden Markov Model by calculating the transitional probabilities (eg probability of verb following a noun) and emission probability(ie how probable is a word given a tag). The transition probablity will be used to predict tag of the missing word/words and emission probability will be used to predict the probability of each word given a tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency Parsing: In a dependency parse tree each word is node in the tree and directed edges represent relations between words. This information is useful in the task of Sentence completion. Instead of calculating $P(w_i| w_i-1)$ we can calcuate probability of $P(w_i | p_i)$ where $p_i$ is parent of word in the depedency tree. For example in the previous example \"I saw a tiger which was really very fierce/talkative\", instead of using P(fierce|very) and P(talkative|very), P(fierce|tiger) and P(talkative|tiger) can be used since 'fierce' and 'tiger', 'talkative' and 'tiger' are connected in the parse tree of our sentence.\n",
    "Given a corpus of data, we get a dependency parse tree of each sentence in the corpus. Now the probability of each pair of words which are connected in the dependency parse tree is calculated. To predict missing words in the test sentence, a dependency parse tree is  created for sentence filled with all possible words.(or words already shortlisted using POS) Probability of missing word is now calculated using the probabilities of the relations we have calculated during the training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Question Answering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a text, the task is to find answers to questions from the given text. Named entities resolution is one of the important step of this task. Lets say we have a large corpus of data. Our system accepts a question from the user then finds the expected named entity of the answer of the question. The system then looks into the corpus and eliminates text that does not mention the expected named entity. In this way we are able to eliminate texts not useful for answering the question. The selected text can then be processed further to extract the relevant answer. The words in the text can be replaced by their named entities for this task.\n",
    "\n",
    "Coreference information can be used in this task to replace each pronoun with the noun it is refering to. Eg in our example \"John met Susan in the mall. She told him that she is traveling to Europe next week.\", the question answering system can be fed with the sentence \"John met Susan in the mall. Susan told John that Susan is traveling to Europe next week.\" after anaphora resolution.\n",
    "\n",
    "The task of question answers also requires word search in the given corpus. It will be useful if different words with same lemma are replaced by their lemma. This would significantly reduce the search space. Eg Consider the text \" She is a caring person. She cares for all. Last year she took care of all the patients\". Here words caring, cares, care have the same lemma 'care'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing CoreNLP and Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John john PROPN NNP nsubj\n",
      "met meet VERB VBD ROOT\n",
      "Susan susan PROPN NNP dobj\n",
      "in in ADP IN prep\n",
      "the the DET DT det\n",
      "mall mall NOUN NN pobj\n",
      ". . PUNCT . punct\n",
      "She -PRON- PRON PRP nsubj\n",
      "told tell VERB VBD ROOT\n",
      "him -PRON- PRON PRP dobj\n",
      "that that ADP IN mark\n",
      "she -PRON- PRON PRP nsubj\n",
      "is be VERB VBZ aux\n",
      "traveling travel VERB VBG ccomp\n",
      "to to ADP IN prep\n",
      "Europe europe PROPN NNP pobj\n",
      "next next ADJ JJ amod\n",
      "week week NOUN NN npadvmod\n",
      ". . PUNCT . punct\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('John met Susan in the mall. She told him that she is traveling to Europe next week.')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy gives 2 POS tags one is simple part-of-speech tag and other detailed part-of-speech tag.\n",
    "1. John is tagged as a proper noun and proper noun singular in Spacy, CoreNLP also tags John as proper noun singular. Same true for met, Susan, in, the, mall, ., She, him, that, she, is, travelling, Europe, week. For words 'to', 'next' both give different results. Spacy tags 'to' as adposition, conjunction whereas CoreNLP gives it a infinitival to tag. Similarly for 'next' Spacy gives Adjective where CoreNLP gives conjunction/preposition.\n",
    "2. Lemma of 'She', 'him' is different in Spacy and CoreNLP. Spacy gives lemma as '-PRON-' whereas CoreNLP give 'she' as the lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get JSON from coreNLP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "sentence = \"Smith ate an apple. It was very tasty. He felt nice. John also ate it, but he did not like it.\"\n",
    "parameters = {\"annotators\":\"tokenize,ssplit,pos,ner,lemma,dcoref,depparse,parse\",\n",
    "              \"outputFormat\":\"json\"}\n",
    "req = requests.Request(method = 'POST', url = 'http://localhost:9000', \n",
    "                     data = sentence,  params = parameters)\n",
    "\n",
    "r = req.prepare()\n",
    "s = requests.session()\n",
    "resp = s.send(r)\n",
    "data = resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part of speech : Vectorized using one hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First step is to get a unique one hot encoding for each tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS_,</th>\n",
       "      <th>POS_.</th>\n",
       "      <th>POS_CC</th>\n",
       "      <th>POS_DT</th>\n",
       "      <th>POS_IN</th>\n",
       "      <th>POS_JJ</th>\n",
       "      <th>POS_NN</th>\n",
       "      <th>POS_NNP</th>\n",
       "      <th>POS_PRP</th>\n",
       "      <th>POS_RB</th>\n",
       "      <th>POS_VBD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    POS_,  POS_.  POS_CC  POS_DT  POS_IN  POS_JJ  POS_NN  POS_NNP  POS_PRP  \\\n",
       "0       0      0       0       0       0       0       0        1        0   \n",
       "1       0      0       0       0       0       0       0        0        0   \n",
       "2       0      0       0       1       0       0       0        0        0   \n",
       "3       0      0       0       0       0       0       1        0        0   \n",
       "4       0      1       0       0       0       0       0        0        0   \n",
       "5       0      0       0       0       0       0       0        0        1   \n",
       "6       0      0       0       0       0       0       0        0        0   \n",
       "7       0      0       0       0       0       1       0        0        0   \n",
       "8       1      0       0       0       0       0       0        0        0   \n",
       "9       0      0       1       0       0       0       0        0        0   \n",
       "10      0      0       0       0       1       0       0        0        0   \n",
       "\n",
       "    POS_RB  POS_VBD  \n",
       "0        0        0  \n",
       "1        0        1  \n",
       "2        0        0  \n",
       "3        0        0  \n",
       "4        0        0  \n",
       "5        0        0  \n",
       "6        1        0  \n",
       "7        0        0  \n",
       "8        0        0  \n",
       "9        0        0  \n",
       "10       0        0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "# Extract all part of speech from the training data\n",
    "all_pos_tags = []\n",
    "for i in range(0, len(data['sentences'])):\n",
    "    for j in range(0, len(data['sentences'][i]['tokens'])):\n",
    "        all_pos_tags.append(data['sentences'][i]['tokens'][j]['pos'])\n",
    "\n",
    "unique_dataframe = []\n",
    "for each in all_pos_tags:\n",
    "    if each not in unique_dataframe:\n",
    "        unique_dataframe.append(each)\n",
    "df = pd.DataFrame(unique_dataframe)\n",
    "vectorized_pos = pd.get_dummies(df, prefix ='POS')\n",
    "vectorized_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next replace all tags in the corpus with their vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize the POS in text\n",
    "vectorized_pos_for_each_word = []\n",
    "for i in range(0, len(data['sentences'])):\n",
    "    for j in range(0, len(data['sentences'][i]['tokens'])):\n",
    "        vectorized_pos_for_each_word.append(vectorized_pos['POS_'+data['sentences'][i]['tokens'][j]['pos']])\n",
    "numpy_vector = np.array(vectorized_pos_for_each_word)\n",
    "numpy_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now if we take the sum along the column, the final vector will give us the count of tags in the text in a vectorized form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 5, 1, 1, 4, 5, 3, 2, 1, 1, 1], dtype=uint64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vector = numpy_vector.sum(axis=0)\n",
    "final_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Each word in the text will be replaced by its lemma. The resulting sentence will be vectorized using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence after replacing words by their lemma's\n",
      "Smith eat a apple .it be very tasty .he feel nice .John also eat it , but he do not like it .\n"
     ]
    }
   ],
   "source": [
    "# replace each word with its lemma\n",
    "pos_word_mapping = {}\n",
    "word_in_sentence = []\n",
    "for i in range(0, len(data['sentences'])):\n",
    "    sentence_words = []\n",
    "    for j in range(0, len(data['sentences'][i]['tokens'])):\n",
    "        sentence_words.append(data['sentences'][i]['tokens'][j]['word'])\n",
    "        if data['sentences'][i]['tokens'][j]['word'] not in pos_word_mapping:\n",
    "            pos_word_mapping[data['sentences'][i]['tokens'][j]['word']] = data['sentences'][i]['tokens'][j]['pos']\n",
    "    word_in_sentence.append(sentence_words)\n",
    "    \n",
    "def get_modified_sentence(word_in_sentence):\n",
    "    modified_sentence = \"\"\n",
    "    for i in range(0, len(word_in_sentence)):\n",
    "        modified_sentence = modified_sentence + ' '.join(word_in_sentence[i])\n",
    "    return modified_sentence\n",
    "for i in range(0, len(data['sentences'])):\n",
    "    for j in range(0, len(data['sentences'][i]['tokens'])):\n",
    "        word_in_sentence[i][j] = data['sentences'][i]['tokens'][j]['lemma']\n",
    "\n",
    "print(\"Sentence after replacing words by their lemma's\")\n",
    "sentence = get_modified_sentence(word_in_sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anaphora resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Each pronoun in the text is replaced by the corresponding NN Noun singular or mass, NNS Noun plural, NNP Proper noun singular, NNPS Proper noun plural. The resulting sentence will be vectorized using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after replacing coreference by the noun it is refering to:\n",
      "Smith eat a apple .apple be very tasty .Smith feel nice .John also eat apple , but John do not like apple .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "coreferences = []\n",
    "for key in data['corefs'].keys():\n",
    "    if len(data['corefs'][key]) > 1:\n",
    "        coreferences.append(data['corefs'][key])\n",
    "nouns = []\n",
    "for i in range(len(coreferences)):\n",
    "    for j in range(0, len(coreferences[i])):\n",
    "        tok = coreferences[i][j]['text'].split()\n",
    "        for each in tok:\n",
    "            if pos_word_mapping[each] in ['NN', 'NNS','NNP','NNPS']:\n",
    "                nouns.append(each)\n",
    "                break\n",
    "\n",
    "for i in range(len(coreferences)):\n",
    "     for j in range(0, len(coreferences[i])):\n",
    "            tok = coreferences[i][j]['text'].split()\n",
    "            if nouns[i] not in tok:\n",
    "                word_in_sentence[coreferences[i][j]['sentNum'] -1][coreferences[i][j]['headIndex']-1] = nouns[i]\n",
    "print(\"Text after replacing coreference by the noun it is refering to:\")\n",
    "print(get_modified_sentence(word_in_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the text can be vectorized using word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to /Users/Aarushi-\n",
      "[nltk_data]     Mac/nltk_data...\n",
      "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.data import find\n",
    "import gensim\n",
    "nltk.download('word2vec_sample')\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0487721   0.0482533  -0.0679696  ... -0.0793844   0.0747147\n",
      "   0.0835352 ]\n",
      " [-0.0557888  -0.0381166  -0.0111751  ... -0.0171524   0.0745005\n",
      "  -0.0183652 ]\n",
      " [-0.0205091  -0.0509621  -0.00384546 ...  0.0435042  -0.00660332\n",
      "   0.109382  ]\n",
      " ...\n",
      " [ 0.0484068  -0.0542491   0.0678809  ... -0.0620387   0.02782\n",
      "  -0.0745577 ]\n",
      " [ 0.0568747   0.075654   -0.00163481 ...  0.0241449  -0.0799465\n",
      "   0.0391684 ]\n",
      " [-0.0205091  -0.0509621  -0.00384546 ...  0.0435042  -0.00660332\n",
      "   0.109382  ]]\n",
      "Shape of the resulting vector (20, 300)\n"
     ]
    }
   ],
   "source": [
    "text_vector = []\n",
    "for i in range(0, len(word_in_sentence)):\n",
    "    for j in range(0, len(word_in_sentence[i])):\n",
    "        if word_in_sentence[i][j] in model.vocab:\n",
    "            text_vector.append(model[word_in_sentence[i][j]])\n",
    "text_vector = np.array(text_vector)\n",
    "print(text_vector)\n",
    "print(\"Shape of the resulting vector\",text_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"John met Susan in the mall. She told him that she is traveling to Europe next week.\"\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(sentence)\n",
    "spacy_result = []\n",
    "word_in_sentence = []\n",
    "for token in doc:\n",
    "    spacy_result.append([token.text, token.lemma_, token.tag_, token.dep_])\n",
    "    word_in_sentence.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['John', 'john', 'NNP', 'nsubj'],\n",
       " ['met', 'meet', 'VBD', 'ROOT'],\n",
       " ['Susan', 'susan', 'NNP', 'dobj'],\n",
       " ['in', 'in', 'IN', 'prep'],\n",
       " ['the', 'the', 'DT', 'det'],\n",
       " ['mall', 'mall', 'NN', 'pobj'],\n",
       " ['.', '.', '.', 'punct'],\n",
       " ['She', '-PRON-', 'PRP', 'nsubj'],\n",
       " ['told', 'tell', 'VBD', 'ROOT'],\n",
       " ['him', '-PRON-', 'PRP', 'dobj'],\n",
       " ['that', 'that', 'IN', 'mark'],\n",
       " ['she', '-PRON-', 'PRP', 'nsubj'],\n",
       " ['is', 'be', 'VBZ', 'aux'],\n",
       " ['traveling', 'travel', 'VBG', 'ccomp'],\n",
       " ['to', 'to', 'IN', 'prep'],\n",
       " ['Europe', 'europe', 'NNP', 'pobj'],\n",
       " ['next', 'next', 'JJ', 'amod'],\n",
       " ['week', 'week', 'NN', 'npadvmod'],\n",
       " ['.', '.', '.', 'punct']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence after word is replaced with lemma using Spacy\n",
      "john meet susan in the mall . -PRON- tell -PRON- that -PRON- be travel to europe next week .\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(word_in_sentence)):\n",
    "    word_in_sentence[i] = spacy_result[i][1]\n",
    "print(\"Sentence after word is replaced with lemma using Spacy\")\n",
    "print(' '.join(word_in_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dict = {}\n",
    "for ent in doc.ents:\n",
    "    ner_dict[ent.text] = ent.label_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence after word is replaced with its named entity using Spacy\n",
      "PERSON meet PERSON in the mall . -PRON- tell -PRON- that -PRON- be travel to LOC next week .\n"
     ]
    }
   ],
   "source": [
    "# replacing each named entities\n",
    "for i in range(len(word_in_sentence)):\n",
    "    if spacy_result[i][0] in ner_dict:\n",
    "        word_in_sentence[i] = ner_dict[spacy_result[i][0]]\n",
    "print(\"Sentence after word is replaced with its named entity using Spacy\")\n",
    "print(' '.join(word_in_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization of the sentence using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.32948613e+00,  1.31929946e+00,  1.48117661e+00,  1.40827036e+00,\n",
       "        4.33249027e-01,  3.18837523e-01, -1.30168188e+00, -1.05192912e+00,\n",
       "        1.28231049e+00,  8.63556027e-01,  8.53094533e-02,  7.18300715e-02,\n",
       "       -2.28958391e-03, -1.61102140e+00,  1.13392644e-01, -5.83186805e-01,\n",
       "       -9.30373132e-01, -1.25734675e+00, -1.14793098e+00,  2.48901650e-01,\n",
       "       -2.39418611e-01, -8.04257572e-01, -2.90454477e-01,  2.39891887e-01,\n",
       "       -1.88766420e-01,  6.62348390e-01,  1.47018284e-01,  1.01151800e+00,\n",
       "       -9.57176983e-01, -1.30864906e+00, -9.61815566e-02, -3.02313417e-01,\n",
       "        6.92688167e-01, -1.09056699e+00, -2.20576331e-01, -8.94120395e-01,\n",
       "        1.61020505e+00,  4.89308685e-02,  7.09146321e-01, -1.24255955e+00,\n",
       "       -1.06523025e+00,  9.00087178e-01,  9.51568007e-01, -1.35924184e+00,\n",
       "       -8.24857533e-01,  1.42789078e+00,  3.76268514e-02, -1.01400010e-01,\n",
       "        1.61380982e+00,  5.48961520e-01,  3.81963521e-01, -6.37452602e-01,\n",
       "       -9.81649905e-02,  6.01710856e-01,  8.14516962e-01,  3.42182338e-01,\n",
       "        1.37977600e+00,  8.35398197e-01,  1.19206026e-01, -3.47341418e-01,\n",
       "       -7.52019167e-01,  9.02650535e-01, -5.38688064e-01, -5.43186543e-05,\n",
       "       -6.00389428e-02, -7.63828903e-02, -4.41755891e-01, -1.15158868e+00,\n",
       "        3.35955173e-01,  5.27576953e-02,  3.35639536e-01, -1.62166691e+00,\n",
       "       -4.07943219e-01,  8.67822766e-01, -1.51617634e+00, -8.00895154e-01,\n",
       "       -5.09239674e-01, -5.89719296e-01, -4.40012276e-01,  4.84725565e-01,\n",
       "       -1.49779069e+00,  1.81179598e-01, -1.34225953e+00, -7.25986660e-01,\n",
       "        1.83240205e-01,  9.80782688e-01, -2.65662849e-01,  1.24189770e+00,\n",
       "        4.49008286e-01,  7.54485309e-01, -9.99138176e-01,  1.82950115e+00,\n",
       "       -6.76453590e-01,  1.22599673e+00, -8.66535366e-01,  2.79413313e-01,\n",
       "       -8.07059705e-01,  3.87847990e-01,  4.09271508e-01, -1.26957810e+00,\n",
       "        2.24200904e-01,  1.13099587e+00,  8.42901111e-01, -4.64542806e-02,\n",
       "       -5.88020563e-01, -3.66398185e-01,  2.35958353e-01, -1.45289254e+00,\n",
       "       -6.19043827e-01,  5.37308194e-02,  1.49098969e+00, -1.37124562e+00,\n",
       "        1.65623918e-01, -9.69080091e-01,  6.23132110e-01,  1.59831524e+00,\n",
       "        5.72071671e-01,  1.03767860e+00,  6.96889833e-02, -7.00065196e-01,\n",
       "        8.53722751e-01,  1.04900444e+00, -5.37229516e-02, -4.74653244e-02,\n",
       "        6.47217095e-01, -9.32033718e-01,  1.17166078e+00, -1.40059149e+00,\n",
       "        5.11716865e-02, -7.45138973e-02, -4.42945883e-02, -2.78023295e-02,\n",
       "       -1.28081903e-01,  1.59743562e-01, -3.24464977e-01,  2.68852681e-01,\n",
       "        1.07762009e-01, -9.16512981e-02, -7.04581011e-03, -1.05748303e-01,\n",
       "        1.57318875e-01, -9.86725315e-02, -9.85486284e-02,  1.14837684e-01,\n",
       "        8.14641491e-02,  2.01069638e-01, -2.74777692e-02, -6.28209412e-02,\n",
       "       -3.10765635e-02,  2.18110681e-01, -6.38652518e-02,  1.67510226e-01,\n",
       "        1.66424707e-01, -1.87049925e-01,  3.39660160e-02, -2.40391031e-01,\n",
       "        1.57155767e-01, -2.99261749e-01, -1.54328346e-01,  1.60774123e-02,\n",
       "        4.79418673e-02,  1.07972123e-01,  2.71087196e-02,  1.41998708e-01,\n",
       "        1.75660804e-01, -2.42682751e-02,  7.93078467e-02, -3.61737087e-02,\n",
       "       -4.70612235e-02,  2.07681470e-02, -1.82376027e-01, -1.60712436e-01,\n",
       "        2.42544606e-01, -2.43277773e-01,  2.62880802e-01,  8.62340406e-02,\n",
       "       -1.61376119e-01,  9.95624345e-03, -1.92935035e-01,  2.99401022e-03,\n",
       "        3.75038162e-02, -1.40983639e-02, -1.23135895e-01,  4.86298911e-02,\n",
       "        3.02292764e-01, -1.67991564e-01,  1.00822791e-01, -1.00049667e-01,\n",
       "        1.17809117e-01,  2.75435388e-01,  1.39348790e-01,  1.67071462e-01,\n",
       "       -9.48652923e-02, -1.17300637e-01, -8.60537440e-02, -2.12310344e-01,\n",
       "       -1.50215551e-01,  2.81651229e-01, -2.89589137e-01, -2.03949451e-01,\n",
       "        1.42216563e-01,  4.62082289e-02,  7.77294189e-02,  1.07033089e-01,\n",
       "       -4.74567823e-02,  8.98545980e-03,  3.27748567e-01,  9.70492605e-03,\n",
       "       -1.85345024e-01, -1.22917779e-01, -1.84196830e-01, -4.77771275e-02,\n",
       "       -3.15179899e-02, -6.45916760e-02, -3.52882296e-01,  3.48374322e-02,\n",
       "       -6.34070039e-02, -1.38592377e-01,  2.50918698e-02, -1.46686718e-01,\n",
       "        1.27705410e-01, -3.16087812e-01, -3.22194457e-01,  1.23202547e-01,\n",
       "       -1.12254836e-01,  4.84660082e-02, -2.26598784e-01,  5.27411094e-03,\n",
       "        1.94604382e-01,  3.26916486e-01,  1.62955984e-01,  1.38335794e-01,\n",
       "        2.71684527e-01, -6.96118623e-02, -9.50217620e-02,  8.84943604e-02,\n",
       "       -1.72060914e-02,  1.31184995e-01, -4.07795794e-02, -3.99972469e-01,\n",
       "        8.62565171e-03, -1.29294693e-01,  7.39680827e-02,  8.75146836e-02,\n",
       "       -1.46916494e-01,  1.65611446e-01,  1.73370883e-01, -3.62930745e-02,\n",
       "       -6.50911257e-02, -3.43054049e-02,  1.65317416e-01, -1.06273152e-01,\n",
       "        2.26326734e-02,  1.07654952e-01,  1.54083073e-01,  3.15807700e-01,\n",
       "       -1.98669165e-01, -9.18437913e-03,  6.34963140e-02,  2.46373832e-01,\n",
       "       -1.06527917e-01, -1.66229531e-01,  1.14430850e-02,  9.46078524e-02,\n",
       "       -1.28721654e-01, -1.58651337e-01, -2.63831168e-01, -1.82386890e-01,\n",
       "       -7.60291889e-02, -1.97439529e-02, -3.82232964e-02,  2.71573871e-01,\n",
       "        1.62878558e-01, -1.68505341e-01, -1.98031381e-01, -2.46712789e-01,\n",
       "       -3.53027172e-02, -3.23182084e-02,  3.09244961e-01, -2.60623842e-01,\n",
       "       -1.05879508e-01,  2.16302760e-02,  6.73274323e-03,  1.38908029e-01,\n",
       "        4.22054455e-02,  9.30531695e-02, -1.61399961e-01, -3.07779223e-01,\n",
       "        1.36937261e-01,  3.17655094e-02,  2.27907762e-01, -1.58433795e-01,\n",
       "        5.11039421e-02,  2.97556221e-01,  1.28467172e-01, -3.09497446e-01,\n",
       "        2.12438285e-01, -3.91938612e-02,  1.34444609e-01, -2.47236684e-01,\n",
       "        1.26518592e-01, -1.71642557e-01, -4.74586301e-02, -2.61408418e-01,\n",
       "        8.81397873e-02, -2.04881236e-01,  1.20632701e-01, -3.42282295e-01,\n",
       "       -1.36912152e-01,  1.99708834e-01, -1.43938363e-01,  1.67534456e-01,\n",
       "       -2.77853012e-02,  2.13109374e-01,  4.30246204e-01, -6.92463964e-02,\n",
       "        1.75025657e-01,  3.24865639e-01,  1.27218097e-01,  2.02628061e-01,\n",
       "        1.65919706e-01,  1.71808466e-01, -2.16358081e-01,  1.98242471e-01,\n",
       "       -3.21686625e-01,  8.16579461e-02, -2.21488494e-02, -1.77101254e-01,\n",
       "       -9.69050080e-02,  2.16460302e-01,  2.93188334e-01, -5.31260855e-02,\n",
       "       -6.81524798e-02,  3.49505186e-01, -9.01442543e-02, -2.96734851e-02,\n",
       "       -2.49754950e-01, -8.60152170e-02, -2.44258836e-01, -1.86977744e-01,\n",
       "       -1.49775982e-01, -5.50441369e-02,  8.22250471e-02, -2.59478614e-02,\n",
       "       -1.80798516e-01, -2.87685007e-01, -1.52535155e-01,  2.50123411e-01,\n",
       "        2.80487835e-01, -8.66000950e-02, -1.95517823e-01,  7.96615034e-02,\n",
       "       -3.25444460e-01,  3.04806560e-01,  2.69893438e-01, -2.25738194e-02,\n",
       "        1.71168037e-02, -1.87883481e-01, -2.34610364e-01, -3.47815692e-01,\n",
       "       -1.16344035e-01,  5.67559376e-02,  2.42922053e-01,  7.61880651e-02,\n",
       "       -3.74131769e-01, -6.97005689e-02,  4.50209938e-02, -7.15538114e-02,\n",
       "        1.48840785e-01, -1.65178662e-03, -2.13096514e-01, -1.81764245e-01,\n",
       "        4.93229516e-02, -7.13146329e-02,  1.89566255e-01,  2.65637785e-01,\n",
       "        1.67186946e-01, -1.53296888e-01,  2.31496334e-01,  1.19957268e-01,\n",
       "       -1.11835778e-01,  1.11241199e-01,  1.01153754e-01, -1.77932307e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(' '.join(word_in_sentence))\n",
    "doc.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a model using fasttext which could classify text from a novel into 'Romantic' or 'Adventure' category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the model\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a skip gram model from the corpus taken from Pride and Prejudice by Jane Austin and adventures of Huckleberry finn by Mark Twain using skipgram function in fastext. CBOW model can aslo be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.skipgram('data.txt', 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model.bin and model.vec will be created. We will now load the model using the load_model function. Then train a supervised classifier using training data in the file 'train_data.txt'. The first word in the training file is the label which should be appended with __label__, followed by the training sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model('model.bin')\n",
    "classifier = fasttext.supervised('train_data.txt', 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now test the classifier on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.75\n",
      "Recall: 0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = classifier.test('test.txt')\n",
    "print(\"Precision:\", result.precision)\n",
    "print(\"Recall:\", result.recall)\n",
    "result.nexamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also give text from the novels and check the predicted tag along with the probability of the predicted tag. The first sentence is from 'Adventure' category and other from 'Romantic'. The classifier predicts them correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Adventure', 0.5)], [('Romantic', 0.5)]]\n"
     ]
    }
   ],
   "source": [
    "texts = ['By and by he rolled out and jumped up on his feet looking wild, and he see me and went for me.', 'With the officers! cried Lydia. I wonder my aunt did not tell us of _that_.']\n",
    "# Or with the probability\n",
    "labels = classifier.predict_proba(texts)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification using Spacy vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performed the same classification task using the same data vectorized by spacy. Used sklearn logistic regression to train the classifier. The classifier gave an accuracy of 91% to identify whether the sentence is from Adventure novel or Romantic novel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(filename):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    with open(filename,\"r\") as f:\n",
    "        for line in f:\n",
    "            splitted_data = line.split(\" \",1)\n",
    "            labels.append(splitted_data[0])\n",
    "            sentences.append(splitted_data[1])\n",
    "    labels_num = []\n",
    "    for e in labels:\n",
    "        if e == '__label__Adventure':\n",
    "            labels_num.append(0)\n",
    "        else:\n",
    "            labels_num.append(1)\n",
    "    vectorized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        vectorized_sentences.append(doc.vector)\n",
    "    return labels_num, vectorized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_num, vectorized_sentences = get_vectors('train_data.txt')\n",
    "labels_num_test, vectorized_sentences_test = get_vectors('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(vectorized_sentences, labels_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of: 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of:\", model.score(vectorized_sentences_test, labels_num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: __label__Adventure  Actual: __label__Adventure \n",
      "Predicted: __label_Romantic  Actual: __label__Adventure \n",
      "Predicted: __label__Adventure  Actual: __label__Adventure \n",
      "Predicted: __label__Adventure  Actual: __label__Adventure \n",
      "Predicted: __label__Adventure  Actual: __label__Adventure \n",
      "Predicted: __label_Romantic  Actual: __label_Romantic \n",
      "Predicted: __label_Romantic  Actual: __label_Romantic \n",
      "Predicted: __label_Romantic  Actual: __label_Romantic \n",
      "Predicted: __label_Romantic  Actual: __label_Romantic \n",
      "Predicted: __label_Romantic  Actual: __label_Romantic \n",
      "Predicted: __label_Romantic  Actual: __label_Romantic \n",
      "Predicted: __label_Romantic  Actual: __label_Romantic \n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict(vectorized_sentences_test)\n",
    "index = 0\n",
    "for sen in vectorized_sentences_test:\n",
    "    print( \"Predicted:\", \"__label__Adventure \" if predicted[index]==0 else \"__label_Romantic \", \"Actual:\",\"__label__Adventure \" if labels_num_test[index]==0 else \"__label_Romantic \")\n",
    "    index = index + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
